{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![IE](../img/ie.png)\n",
    "\n",
    "# Sessions 9 & 10: Multi-process and asynchronous programming\n",
    "\n",
    "### Juan Luis Cano Rodríguez <jcano@faculty.ie.edu> - Master in Business Analytics and Big Data (2020-01-16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First rule of performance analysis\n",
    "\n",
    "> \"We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil.\"\n",
    ">\n",
    "> — Donald E. Knuth\n",
    "\n",
    "> **premature** (adj.) pre·ma·ture : happening, arriving, existing, or performed before the proper, usual, or intended time\n",
    ">\n",
    "> Merriam-Webster dictionary\n",
    "\n",
    "There are several techniques to make slow programs go faster. However, no matter what we do, the first step is to **analyze** _where does the slowdown come from_.\n",
    "\n",
    "Among the most straight-forward tools to analyze performance in Python are cProfile and line_profiler, and they are very easy to use from the notebook. cProfile is part of the standard library, whereas line_profiler must be installed with pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install line_profiler  # Python <= 3.6\n",
    "!pip install cython  # Required to install line_profiler from source\n",
    "!pip install https://github.com/rkern/line_profiler/archive/master.zip  # Required in Python 3.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simple example: a function to estimate the value of $\\pi$ using a Monte Carlo simulation:\n",
    "\n",
    "![Monte Carlo pi](../img/monte_carlo_pi.png)\n",
    "\n",
    "(Source: https://towardsdatascience.com/speed-up-jupyter-notebooks-20716cbe2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def estimate_pi(num_sim=1e7):\n",
    "    \"\"\"Estimate pi with monte carlo simulation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_sim: int\n",
    "        Number of simulations.\n",
    "\n",
    "    \"\"\"\n",
    "    in_circle = 0\n",
    "    ii = 0\n",
    "    while ii < num_sim:\n",
    "        prec_x = np.random.rand()\n",
    "        prec_y = np.random.rand()\n",
    "\n",
    "        # Let's use pow here instead of **\n",
    "        # to see it in the cProfile report\n",
    "        if pow(prec_x, 2) + pow(prec_y, 2) <= 1:\n",
    "            in_circle += 1  # inside the circle\n",
    "\n",
    "        ii += 1\n",
    "        \n",
    "    return 4 * in_circle / num_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running it through `cProfile` will tell us which functions or methods are called more. For that, we have to write `%prun` in front of the executable code, adding `-s cumtime` to sort by cumulative time. Notice that this will only work in IPython and Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This opens a separate panel with information similar to this:\n",
    "\n",
    "```\n",
    "         4000004 function calls in 3.821 seconds\n",
    "\n",
    "   Ordered by: cumulative time\n",
    "\n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "        1    0.000    0.000    3.821    3.821 {built-in method builtins.exec}\n",
    "        1    0.000    0.000    3.821    3.821 <string>:1(<module>)\n",
    "        1    2.048    2.048    3.821    3.821 <ipython-input-87-d37e0e33fe4b>:4(estimate_pi)\n",
    "  2000000    1.296    0.000    1.296    0.000 {method 'rand' of 'mtrand.RandomState' objects}\n",
    "  2000000    0.478    0.000    0.478    0.000 {built-in method builtins.pow}\n",
    "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the information shown by `cProfile` gives a useful hint because it's a summary of the whole execution. However, other times it's better to profile the program line by line, and visually identifying the hotspots. For that, we have to load the `line_profiler` IPython extension and run the code with `%lprun`, adding `-f <my_function` for all the functions I want to display in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This again opens a separate panel with information similar to this:\n",
    "\n",
    "```\n",
    "Timer unit: 1e-06 s\n",
    "\n",
    "Total time: 8.75213 s\n",
    "File: <ipython-input-2-d37e0e33fe4b>\n",
    "Function: estimate_pi at line 4\n",
    "\n",
    "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
    "==============================================================\n",
    "     4                                           def estimate_pi(num_sim=1e7):\n",
    "     5                                               \"\"\"Estimate pi with monte carlo simulation.\n",
    "     6                                               \n",
    "     7                                               Parameters\n",
    "     8                                               ----------\n",
    "     9                                               num_sim: int\n",
    "    10                                                   Number of simulations.\n",
    "    11                                           \n",
    "    12                                               \"\"\"\n",
    "    13         1          9.0      9.0      0.0      in_circle = 0\n",
    "    14         1          6.0      6.0      0.0      ii = 0\n",
    "    15   1000001    1074014.0      1.1     12.3      while ii < num_sim:\n",
    "    16   1000000    1916723.0      1.9     21.9          prec_x = np.random.rand()\n",
    "    17   1000000    1812770.0      1.8     20.7          prec_y = np.random.rand()\n",
    "    18                                           \n",
    "    19                                                   # Let's use pow here instead of **\n",
    "    20                                                   # to see it in the cProfile report\n",
    "    21   1000000    1947163.0      1.9     22.2          if pow(prec_x, 2) + pow(prec_y, 2) <= 1:\n",
    "    22    785268     897605.0      1.1     10.3              in_circle += 1  # inside the circle\n",
    "    23                                           \n",
    "    24   1000000    1103842.0      1.1     12.6          ii += 1\n",
    "    25                                                   \n",
    "    26         1          3.0      3.0      0.0      return 4 * in_circle / num_sim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of parallelism\n",
    "\n",
    "In Python there are two parallelism models:\n",
    "\n",
    "* **Multithreading**: There is one single process that can have several execution _threads_ at the same time. The (theoretical) advantage is that all these threads share the same memory and are lightweight.\n",
    "* **Multiprocessing**: Several processes are launched at the same time. The disadvantage is that they have more overhead and it's more difficult to share data between them.\n",
    "\n",
    "It seems clear that multithreading would be the obvious choice, but in Python it's not the case.\n",
    "\n",
    "We can broadly classify computer programs in two groups:\n",
    "\n",
    "* **CPU-bound**: The bottleneck is the CPU. The faster the CPU, the faster the program will complete. Basically any calculation the computer does with data that resides in RAM.\n",
    "* **I/O-bound**: The bottleneck is the input/output, either from disk, from the network, or any other sources. The faster the I/O, the faster the program will complete. This includes downloading data from the Internet, reading files from disk, and so forth.\n",
    "\n",
    "The Python canonical implementation (CPython, the one everybody uses) has a limitation called the Global Interpreter Lock (GIL), which means that **only one thread can use the CPU at a time**. Therefore, as a rule of thumb, **multithreading should not be used in Python for CPU-bound code**.\n",
    "\n",
    "For the rest of this course, even though multithreading can be used in Python for I/O-bound code, we will skip it entirely, since it's also more complex to synchronize and can lead to subtle errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "The number of processes we run in parallel shouldn't be higher than the number of CPUs we have, and can be obtained with the `multiprocessing.cpu_count` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to leverage `multiprocessing` is to create a `Pool` with a pre-defind number of processes that will run in parallel in our computer, and that has convenience methods that allow us to call a function with a sequence of arguments in parallel:\n",
    "\n",
    "<div class=\"alert alert-warning\">Under certain circumstances, directly applying <code>multiprocessing.Pool</code> to some code in the notebook might not work, see <a href=\"https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac\">this explanation</a>. In that case, it should be enough to move the function we want to parallelize to a <code>.py</code> module, or wrap the execution inside a <code>if __name__ == \"__main__\":</code> block.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's _almost_ four times faster! But why doesn't it take exactly 1 second? The reason is mostly **process overhead**. If the process is very fast, the time it taks to spawn a new process becomes the bottleneck. See what happens if we remove the `sleep`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It's actually 30 000 times slower!* That gives us an idea of what is the process overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a Monte Carlo simulation that leverages `multiprocessing` to accelerate the estimation of $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous programming\n",
    "\n",
    "Asynchronous programming is different from parallelism, and is in fact a form of **concurrency**. The differences between parallelism and concurrency are subtle - for example, from [this Stack Overflow answer](https://stackoverflow.com/a/36604522/554319):\n",
    "\n",
    "> An application can be concurrent – but not parallel, which means that it processes more than one task at the same time, but no two tasks are executing at same time instant.\n",
    "\n",
    "> Concurrency is about _dealing_ with lots of things at once. Parallelism is about _doing_ lots of things at once. [Emphasis mine]\n",
    "\n",
    "Before going any further, let's do a simple I/O-bound program and then we will accelerate it using `asyncio`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In asynchronous programming there is an **event loop** that runs **coroutines**. Every time a coroutine is **_awaited_**, the event loop jumps to another coroutine. It's a way of saying \"this will take some time, so start doing it and I will continue with other things\".\n",
    "\n",
    "The way to define coroutines is adding `async` before `def`, and the way to schedule them is using `asyncio.create_task`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how, after the last line of the cell, the kernel was not blocked anymore and all the coroutines started at once, so the total running time was sorter than the sum of running times. This is possible because Jupyter has an event loop already running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the disadvantage is that our asynchronous code in Jupyter will behave very different outside it. In particular, you will notice that, the \"hello world\" example from the asyncio documentation does not work in Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.python.org/3.7/library/asyncio.html\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    print('Hello ...')\n",
    "    await asyncio.sleep(1)\n",
    "    print('... World!')\n",
    "\n",
    "# Python 3.7+\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, avoid developing async code in Jupyter unless you know exactly what you're doing. The snippet above works fine in a normal Python or IPython shell, and in a `.py` script. For more details, see [the technical explanation by IPython maintainer](https://blog.jupyter.org/ipython-7-0-async-repl-a35ce050f7f7)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data Science in Python with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
